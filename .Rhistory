mean(y) # Central tendency, mean
# here is where you load any necessary packages
# ex: stringr
# lapply(c("stringr"),  pkgTest)
lapply(c(),  pkgTest)
#####################
# load libraries
# set wd
# clear global .envir
#####################
# Get working directory
getwd()
# Set working directory
setwd("/Users/carty/OneDrive/Documents/Stats1 Fall Prep")
getwd()
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# here is where you load any necessary packages
# ex: stringr
# lapply(c("stringr"),  pkgTest)
lapply(c(),  pkgTest)
#####################
# Problem 1
#####################
# QUESTION 1: find a 90% Confidence Interval for the average IQ in the school
# below is a random sample of 25 students' IQ scores
y <- c(105, 69, 86, 100, 82, 111, 104, 110, 87, 108, 87, 90, 94, 113, 112, 98, 80, 97, 95, 111, 114, 89, 95, 126, 98)
# let's look at key data
sort(y) # reorder scores
mean(y) # Central tendency, mean
var(y) # Variability, variance
sd(y) # Variability, standard deviation
sd(y) / sqrt(length(y))  # Variability, standard **error**
summary(y)
standard_error <-  sd(y) / sqrt(length(y))
standard_error
mean(y) - 100 / standard_error
(mean(y) - 100) / standard_error
t_score <- (mean(y) - 100) / standard_error
t_score
pt(t_score, (length(y)-1))
prob <- pt(t_score, (length(y)-1))
prob
# Find T statistic
t_statistic <- (mean(y) - 100) / standard_error
# Find probability score
prob <- pt(t_statistic, (length(y)-1))
prob <- pt(t_statistic, (length(y)-1))
prob
t.test(y, mu = 100, alternative = "greater")
t_statistic
prob
prob <- pt(t_statistic, 24)
prob
prob <- pt(t_statistic, (length(y)-1))
prob
t.test(y, mu = 100, conf.level = 0.095, alternative = "greater")
t_statistic
t_statistic
t_statistic
t_statistic <- abs((mean(y) - 100) / standard_error)
t_statistic
prob <- pt(t_statistic, (length(y)-1))
prob
P_value <- pt(q = -0.59574, df = (length(y)-1), lower.tail = FALSE)
P_value
t.test(y, mu= 100)
t.test(y, mu = 100, conf.level = 0.05, alternative = "greater")
t.test(y, mu = 100, conf.level = 0.10)
t.test(y, mu = 100, conf.level = 0.10)
t.test(y, mu = 100, conf.level = 0.05, alternative = "greater")
t.test(y, mu = 100, conf.level = 0.05, alternative = "greater")
setwd("/Users/carty/OneDrive/Documents/GitHub/StatsI_Fall2023/problemSets/PS01/my_answers")
# Get working directory
getwd()
# Set working directory
setwd("/Users/carty/OneDrive/Documents/GitHub/StatsI_Fall2023/problemSets/PS01/my_answers")
getwd()
#####################
# load libraries
# set wd
# clear global .envir
#####################
# Get working directory
getwd()
# Set working directory
setwd("/Users/carty/OneDrive/Documents/GitHub/StatsI_Fall2023/problemSets/PS01/my_answers")
getwd()
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# here is where you load any necessary packages
# ex: stringr
# lapply(c("stringr"),  pkgTest)
lapply(c(),  pkgTest)
#####################
# Problem 1
#####################
# QUESTION 1: find a 90% Confidence Interval for the average IQ in the school
# below is a random sample of 25 students' IQ scores
y <- c(105, 69, 86, 100, 82, 111, 104, 110, 87, 108, 87, 90, 94, 113, 112, 98, 80, 97, 95, 111, 114, 89, 95, 126, 98)
# let's look at key data
sort(y) # reorder scores
mean(y) # Central tendency, mean
var(y) # Variability, variance
sd(y) # Variability, standard deviation
sd(y) / sqrt(length(y))  # Variability, standard **error**
summary(y)
# Some quick visualizations, to look at distribution
hist(y,
breaks = 5,
main = "Students' IQ scores",
xlab = "IQ score")
plot(y,
main="Students' IQ scores",
ylab="IQ scores")
#To find the 90% Confidence Interval, we look for the Point estimate +/- Margin of error,
# where margin of error is a multiple of the standard error
# What do we need?
mean(y) # Central tendency, mean - a point estimate
sd(y) / sqrt(length(y))  # Variability, the standard error
# How to find the multiple?
# Looking at the normal distribution, we see that 90% of observations lie within +/-1.64
# standard errors of the point estimate
?qnorm
qnorm(0.05) # value for first 5%
qnorm(0.95) # value last 5%
# Solution method 1: The **approximate** solution for 90% confidence level
upper_90 = mean(y)+(1.64*sd(y) / sqrt(length(y)))
lower_90 = mean(y)-(1.64*sd(y) / sqrt(length(y)))
#Summary
lower_90
mean(y)
upper_90
# SOlution method 2: The **precise** solution, using normal distribution for 90% confidence level
lower_90_n <- qnorm(0.05,
mean = mean(y),
sd = (sd(y) / sqrt(length(y))))
# Upper bound, 95 confidence level
upper_90_n <- qnorm(0.95,
mean = mean(y),
sd = (sd(y) / sqrt(length(y))))
standard_error <-  sd(y) / sqrt(length(y))  # Variability, standard error
# Find T statistic
t_statistic <- abs((mean(y) - 100) / standard_error)
# Find probability score
prob <- pt(t_statistic, (length(y)-1))
t.test(y, mu = 100, conf.level = 0.05, alternative = "greater")
hist(y,
breaks = 5,
abline(v=mean(y),col="black"),
main = "Students' IQ scores",
xlab = "IQ score")
abline(v=mean(y),col="black")
abline(v=lower_90,col="black",lty="dashed")
abline(v=upper_90,col="black",lty="dashed")
plot(expenditure$Y, expenditure$Region)
plot(expenditure$Y, expenditure$X1)
expenditure <- read.table("https://raw.githubusercontent.com/ASDS-TCD/StatsI_Fall2023/main/datasets/expenditure.txt", header=T)
plot(expenditure$Y, expenditure$X1)
plot(expenditure$Y, expenditure$X2)
plot(expenditure$Y, expenditure$X3)
plot(expenditure$X1, expenditure$X2)
plot(expenditure$X1, expenditure$X3)
plot(expenditure$X2, expenditure$X3)
# Get working directory
getwd()
# Set working directory
setwd("/Users/carty/OneDrive/Documents/GitHub/StatsI_Fall2023")
getwd()
df_not_tidy <- read.csv("datasets/movies.csv")
df_not_tidy <- read.csv("datasets/movies.csv")
# First step, look at data
View(df_not_tidy)
str(df_not_tidy)
head(df_not_tidy)
summary(df_not_tidy)
View(df_not_tidy)
str(df_not_tidy)
head(df_not_tidy)
summary(df_not_tidy)
df <- readRDS("datasets/movies.rds")
str(df)
class(df$genre)
levels(df$genre)
table(df$genre, # Genre
df$critics_rating) # Rating
df_s <- df[df$genre=="Comedy" |
df$genre=="Drama" |
df$genre=="Documentary", ]
View(df_s)
if(!require(tidyverse)){
install.packages("tidyverse")
library(tidyverse)
}
df_s <- subset(df, df$genre %in% c("Comedy","Documentary","Drama"))
View(df_s)
?subset
View(df_s)
View(df_s)
levels(df_s$genre)
df_s$genre <- droplevels(df_s$genre)
View(df_s)
View(df_s)
# Contingency table
table(df_s$genre, # Genre
df_s$critics_rating) # Rating
# Add marginal distributions
addmargins(table(df_s$genre, # Genre
df_s$critics_rating)) # Rating
# Joint probability
prop.table(table(df_s$genre,
df_s$critics_rating))
?prop.table()
prop.table(table(df_s$genre, # rows
df_s$critics_rating), # columns
margin = 1) # over rows
63/87
# Over rows --> Rating conditional on genre
addmargins(prop.table(table(df_s$genre,
df_s$critics_rating),
margin = 1)) # over rows
round(addmargins(prop.table(table(df_s$genre,
df_s$critics_rating),
margin = 1)), 2)
# Over columns --> Genre conditional on rating
addmargins(prop.table(table(df_s$genre, # rows
df_s$critics_rating), # columns
margin = 2)) # over columns
63/190
barplot(prop.table(table(df_s$genre,
df_s$critics_rating), margin=1),
xlab="Ranking",
ylab="Proportions",
main="Critics Rating by Genre",
beside=TRUE,
legend.text = TRUE,
args.legend = list(x=12,
y=0.7,
cex = 0.8,
box.col = "white"))
png(filename = "tutorials/03/barplot.png",
width = 600,
height = 350)
barplot(prop.table(table(df_s$genre,
df_s$critics_rating),margin=1),
xlab="Ranking",
ylab="Proportions",
main="Critics Rating by Genre",
beside=TRUE,
legend.text = TRUE,
args.legend = list(x=12,
y=0.7,
cex = 0.8,
box.col = "white"))
dev.off()
chisq.test(df_s$genre,
df_s$critics_rating)
sprintf("%.20f",1.097e-12)
